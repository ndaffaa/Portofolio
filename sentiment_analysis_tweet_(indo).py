# -*- coding: utf-8 -*-
"""Sentiment Analysis Tweet (indo).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gEJUg47gqvgGAhG3a0lU9-9mZReKz5V1
"""

#Import library needed

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
import tensorflow as tf
import seaborn as sn

from numpy import array
from sklearn.model_selection import train_test_split

from tensorflow.keras import regularizers
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

#Load dataset
data = pd.read_csv('dataset.csv', index_col=0)
data.head()

#Preprocessing

y = [0 if i=='negative' else 1 for i in data['Sentiment']]
x = data['Text Tweet']

#Spliting

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)

(unique, counts) = np.unique(y_test, return_counts=True)
frequencies_test = np.asarray((unique, counts)).T

(unique, counts) = np.unique(y_train, return_counts=True)
frequencies_train = np.asarray((unique, counts)).T
print(frequencies_test)
print(frequencies_train)

vocab_size = 1000
embedding_dim = 16
max_length = 100
trunc_type = 'post'
padding_type = 'post'
oov_tok = "<OOV>"

#Tokenization
tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(x)

word_index = tokenizer.word_index

training_sequences = tokenizer.texts_to_sequences(x_train)
testing_sequences = tokenizer.texts_to_sequences(x_test)

#Padding
training_padded  = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)
testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

training_padded = np.array(training_padded)
training_labels = np.array(y_train)

testing_padded = np.array(testing_padded)
testing_labels = np.array(y_test)

model = tf.keras.Sequential([
                             tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
                             tf.keras.layers.GlobalAveragePooling1D(),
                             tf.keras.layers.Dense(24, activation='relu'),
                             tf.keras.layers.Dropout(0.05),
                             tf.keras.layers.Dense(1, activation='relu')
])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

num_epochs = 100
history=model.fit(training_padded, training_labels, batch_size=32, epochs=num_epochs, validation_data=(testing_padded, testing_labels))

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_' + string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, "val_"+string])
  plt.show()

plot_graphs(history, 'accuracy')
plot_graphs(history, 'loss')

#test sentence

sentence = ['filmnya lucu banget, gua ngakak']
sequences = tokenizer.texts_to_sequences(sentence)
padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)
print(np.rint(model.predict(padded)))

